#version: '3.8'

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
    ports:
      - "2181:2181"
    volumes:
      - zk-data:/var/lib/zookeeper/data
      - zk-log:/var/lib/zookeeper/log
    networks:
      - spark-network

  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
      - "29092:29092"
    healthcheck:
      test: ["CMD", "kafka-topics", "--bootstrap-server", "localhost:9092", "--list"]
      interval: 10s
      timeout: 10s
      retries: 10
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,PLAINTEXT_HOST://0.0.0.0:29092
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_LOG_RETENTION_HOURS: 168
      KAFKA_LOG_CLEANUP_POLICY: compact
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_ENABLE_IDEMPOTENCE: "true"
      KAFKA_FETCH_MAX_BYTES: 52428800
      KAFKA_SOCKET_REQUEST_MAX_BYTES: 52428800
      KAFKA_REPLICA_FETCH_MAX_BYTES: 52428800
    volumes:
      - kafka-data:/var/lib/kafka/data
    networks:
      - spark-network

  schema-registry:
    image: confluentinc/cp-schema-registry:7.6.0
    depends_on:
      - kafka
    ports:
      - "8012:8081"
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: PLAINTEXT://kafka:9092
    networks:
      - spark-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081/subjects"]
      interval: 10s
      timeout: 5s
      retries: 5

  kafka-ui:
    image: provectuslabs/kafka-ui
    ports:
      - "8080:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9092
      KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181
      KAFKA_CLUSTERS_0_PROPERTIES_SECURITY_PROTOCOL: PLAINTEXT
      KAFKA_CLUSTERS_0_CONSUMER_PROPERTIES_GROUP_ID: kafka-ui-group
    networks:
      - spark-network

  create-topic:
    build:
      context: ./topic_admin
    container_name: topic-admin
    depends_on:
      kafka:
        condition: service_healthy
      schema-registry:
        condition: service_healthy
    networks:
      - spark-network


  clickstream-producer:
    build:
      context: ./clickstream_producer
    container_name: clickstream-producer
    depends_on:
      schema-registry:
        condition: service_healthy
      create-topic:
        condition: service_completed_successfully
    networks:
      - spark-network

  spark-master:
#    image: bitnami/spark:3.3.1
    build:
      context: .
      dockerfile: spark/Dockerfile
    deploy:
      resources:
        limits:
          memory: 2g # Or more if your driver is memory intensive
          cpus: '1'
#    volumes:
#      - ./spark/jars:/opt/bitnami/spark/jars

    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_WEBUI_PORT=9080
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    ports:
      - "9080:9080"
      - "7077:7077"
    networks:
      - spark-network

  spark-worker-1:
#    image: bitnami/spark:3.3.1
    build:
      context: .
      dockerfile: spark/Dockerfile
    deploy:
      resources:
        limits:
          memory: 5g  # Provide more than 4GB for the container
          cpus: '2'   # Provide at least 2 CPUs
#    volumes:
#      - ./spark/jars:/opt/bitnami/spark/jars

    container_name: spark-worker-1
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=4G
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_WEBUI_PORT=9081
    depends_on:
      - spark-master
    ports:
      - "9081:9081"
    networks:
      - spark-network

  spark-worker-2:
#    image: bitnami/spark:3.3.1
    build:
      context: .
      dockerfile: spark/Dockerfile
    deploy:
      resources:
        limits:
          memory: 5g  # Provide more than 4GB for the container
          cpus: '2'   # Provide at least 2 CPUs
#    volumes:
#      - ./spark/jars:/opt/bitnami/spark/jars
    container_name: spark-worker-2
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=4G
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_WEBUI_PORT=9082
    depends_on:
      - spark-master
    ports:
      - "9082:9082"
    networks:
      - spark-network

#  jupyter:
#    image: jupyter/pyspark-notebook:spark-3.3.1
#    container_name: jupyter-spark
#    environment:
#      - PYSPARK_SUBMIT_ARGS=--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1,org.apache.spark:spark-avro_2.12:3.3.1,io.confluent:kafka-schema-registry-client:7.5.0,io.confluent:kafka-avro-serializer:7.5.0 --master spark://spark-master:7077 pyspark-shell
#    ports:
#      - "8888:8888"
#    networks:
#      - spark-network
#    volumes:
#      - ./notebooks:/home/jovyan/work
#    depends_on:
#      - spark-master

  airflow-webserver:
    image: apache/airflow:2.9.1-python3.10
    build:
      context: ./airflow
      dockerfile: Dockerfile # <--- Assuming your Airflow Dockerfile is named 'Dockerfile' within 'airflow'
    container_name: airflow-webserver
    restart: always
    ports:
      - "8081:8080"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - airflow-db-volume:/opt/airflow/
      - ./parquet_output:/opt/airflow/parquet_output
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      AIRFLOW__CORE__EXECUTOR: SequentialExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: sqlite:////opt/airflow/airflow.db
      AIRFLOW__CORE__FERNET_KEY: ''
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    networks:
      - spark-network
    depends_on:
      - spark-master
      - kafka
    command: >
      bash -c "
        airflow db init &&
        airflow users create --username admin --password admin --firstname Naman --lastname Goyal --role Admin --email namangoyal1000@gmail.com &&
        airflow webserver
      "

  airflow-scheduler:
    image: apache/airflow:2.9.1-python3.10
    build:
      context: ./airflow
      dockerfile: Dockerfile # <--- Assuming your Airflow Dockerfile is named 'Dockerfile' within 'airflow'
    container_name: airflow-scheduler
    restart: always
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - airflow-db-volume:/opt/airflow/
      - ./parquet_output:/opt/airflow/parquet_output
      - /var/run/docker.sock:/var/run/docker.sock

    environment:
      AIRFLOW__CORE__EXECUTOR: SequentialExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: sqlite:////opt/airflow/airflow.db
    networks:
      - spark-network
    depends_on:
      - airflow-webserver
    command: >
          bash -c "
            while [ ! -f /opt/airflow/airflow.db ]; do
              echo 'Waiting for airflow.db to be initialized...';
              sleep 3;
            done &&
            airflow scheduler
          "

  minio:
    image: minio/minio
    ports:
      - "9000:9000"
      - "9001:9001"
    networks:
      - spark-network
    volumes:
      - minio-data:/data
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    command: server /data --console-address ":9001"

  minio-client:
    image: minio/mc
    networks:
      - spark-network
    depends_on:
      - minio
    entrypoint: >
      /bin/sh -c "
        sleep 5;
        mc alias set local http://minio:9000 minioadmin minioadmin;
        mc mb -p local/iceberg;
        mc policy set public local/iceberg;
        exit 0;
      "


networks:
  spark-network:
    driver: bridge

volumes:
  kafka-data:
  zk-data:
  zk-log:
  airflow-db-volume:
  minio-data:
